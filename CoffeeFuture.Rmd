---
title: 'Coffee Future Price Pediction Project: ICFFUT B3 Dataset'
author: "Mauricio Rabelo Soares"
date: "03 set 2022"
output:
  pdf_document:
    latex_engine: lualatex
  word_document: default
  html_document:
    df_print: paged
subtitle: "HarvardX - PH125.9x Data Science: Capstone Course"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

A Machine Learning for Factor Investing is a equity **investment strategies** that are built on **firm characteristics**, the goal of this approach is to determine the model that maps the time-t characteristics of firms to their future performance [@coqueret2020a].[^1] Its premise is that differences in the returns of firms can be explained by the characteristics of these firms. In this project we use a similar approach but, instead a firm performance, our outcome is the trade decision taken at close call of 4/5 Arabica Coffee Futures contract traded at B3, which ticker is ICF.[^2] The goal of this project is to create a decision trade system which balanced accuracy, or F1 score, is above 50%, [^3]using all the tools we have learn throughout the multi-part course in HarvardX's Data Science Professional Certificate series. For this challenge we going to use a dataset provided by the Profit Trader Clear[^4]. The dataset begin in January 2003 and has **4435 decisions** applied to **1 asset** with **20 features**.

[^1]: <http://www.mlfactor.com/index.html>

[^2]: <https://www.b3.com.br/en_us/products-and-services/trading/commodities/product-sheet-8AE490C96D41D3A2016D46017EC97262.htm>

[^3]: "The mean squared error is usually hard to interpret. It's not easy to map an error on returns into the impact on investment decisions. The hit ratio is a more intuitive indicator because it evaluates the proportion of correct guesses (and hence profitable investments). Obviously, it is not perfect: 55% of small gains can be mitigated by 45% of large losses. Nonetheless, it is a popular metric and moreover it corresponds to the usual accuracy measure often computed in binary classification exercises. Here, an accuracy of 0.542 is satisfactory. Even if any number above 50% may seem valuable, it must not be forgotten that transaction costs will curtail benefits. Hence, the benchmark threshold is probably at least at 52%." (Coqueret and Guida 2020)

[^4]: <https://corretora.clear.com.br/plataformas/profit-trader-clear/>

# Methods

The methods section explains the process and techniques used in this project, in the first part, then explains the data cleaning used to extract and clean the data, in the second part. In the third part of this section we present the data exploration and visualization of the data to gain some insights. The fourth, and last part of this section, we show the modeling approach used in this project.

## The process and techniques used

The decision trade system is similar to a **decision support system** (**DSS**) which is an interactive software-based system intended to help decision makers (future contract trader) compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions[^5]. The future trader decisions could be **buy** the contract, **sell** or not get in a position, what is the same to be **neutral** in a asset, this decisions will be our **outcome** $\mathbf{y}=y_t$, which is categorical.

[^5]: <https://en.wikipedia.org/wiki/Decision_support_system>

Every trade decision taken, in a future market, occur in a certain time $t$, in a specific contract and organized marked, for a unique value. For this project the contract is the 4/5 Arabica Coffee Futures traded at B3, which ticker is ICF, and are available for march, may, july, september and december. The ICFFUT is the representative time-series of current contract and some of thus features will be our the dataset. The dimension of the feature matrix $X$ is $T×K$: where $T$ are daily **observations** and each one of them has $K$ **features**, **inputs**, or **predictors** which will serve as **independent** and **explanatory** variables.

The decision trade system build in this project are made using a similar process and techniques presented at [@coqueret2020a] and some approaches used by Renaissance Technologies (Zuckerman, 2019).[^6] The backtesting protocol propose by [@arnott2018] are used as reference to evaluate the model.

[^6]: <https://www.youtube.com/watch?v=lji-jNsXmAM>

    Renaissance Technologies - Trading Strategies Revealed \| A Documentary

The process begin with the download, then read the data and create some extensions using the Close price. After the data cleaning we explore the data to gain some insight throughout visualization and selected table. This insights is the basis of the modelling approach of this project, that have 3 non linear algorithms: k-Nearest Neighbour Classification, Recursive Partitioning and Classification Trees, and Classification and Regression with Random Forest

## Data cleaning

For this project we going to use a subset of dataset provided by Profit Trader Clear, and for the sake of **reproducibility**, we will illustrate the concepts based on a single financial dataset available at <https://github.com/mrabelosoares/Coffee-Future-Price-Prediction>. This dataset comprises information on 1 contract listed at B3, which the time range starts in January 2003 and ends in August 2022. For each point in time, 19 **characteristics** describe the decision in the sample. The dataset are divide between raw data and extensions:

**Raw data**: Date; Asset; Price (Open, High, Low, Close); Volume; Decision; Open Interest

```{r Load Library, message=FALSE, warning=FALSE, echo=TRUE}

library(caret)
library(data.table)
library(fields)
library(tidyverse)
library(knitr)
library(kableExtra)
library(grid)
library(ggplot2)
library(lattice)
library(gridExtra)
library(readxl)
library(dplyr)
library(purrr)
library(zoo)
library(runner)
library(quantmod)
library(rpart)
library(randomForest)
library(xts)
library(TTR)
library(corrplot)
library(rpart.plot)
```

```{r Creat and clean the dataset, message=FALSE, warning=FALSE, echo=TRUE}
##Data Clean

#create tempfile and download
dl <- tempfile()
download.file("https://github.com/mrabelosoares/Coffee-Future-Price-Prediction/blob/2044135ec7c5bcfb6f357374de4ede11c9382fd8/CoffeDatabase.xlsx", dl)
#read file XLSX format with decisions
ICFFUT <- read_xlsx("CoffeDatabase.xlsx", sheet = "ICFFUT")
#read file XLSX format with ICF Prices
ICFFUT_XLSX <- read_xlsx("CoffeDatabase.xlsx", sheet = "ICFFUT_XTS")
#Convert XLXS format to Data Frame
DFICFFUT_XTS <- as.data.frame(ICFFUT_XLSX)
#Convert Data Frame to XTS
ICFFUT_XTS <- xts(DFICFFUT_XTS[-1], order.by = as.Date(DFICFFUT_XTS$Date))
```

**Extensions**: moving average 5 days; moving average 22 days; Bollinger Bands; Rate of Change Oscillator; Relative Strength Index; Stochastic Momentum Index; MACD Oscillator. The description and equations of technical indicator based on price are available at reference manual: <https://cran.r-project.org/web/packages/TTR/TTR.pdf>

```{r Technical Indicators - Price-Based, message=FALSE, warning=FALSE, echo=TRUE}
##Technical Indicators - Price-Based
#moving average 5 days
Moving_Average_5 <- SMA(ICFFUT_XTS$ICFFUT.Close, n=5)
#moving average 22 days
Moving_Average_22 <- SMA(ICFFUT_XTS$ICFFUT.Close, n=22)
#Bollinger Bands
Bollinger_Bands <- BBands(ICFFUT_XTS$ICFFUT.Close)
#Rate of Change Oscillator
Rate_Change_Oscillator <- ROC(ICFFUT_XTS$ICFFUT.Close, n=10)
#Relative Strength Index
Relative_Strength_Index <- RSI(ICFFUT_XTS$ICFFUT.Close)
#Stochastic Oscillator / Stochastic Momentum Index
Stochastic <- stoch(ICFFUT_XTS$ICFFUT.Close)
#MACD Oscillator
MACD <- MACD(ICFFUT_XTS$ICFFUT.Close)
```

The raw data and the extensions are merged to create the complete data frame.

```{r merge raw data and the extensions, message=FALSE, warning=FALSE, echo=TRUE}
#merge XTS
Full_ICFFUT <- merge(ICFFUT_XTS, 
                     Moving_Average_5, 
                     Moving_Average_22,
                     Bollinger_Bands,
                     Rate_Change_Oscillator,
                     Relative_Strength_Index,
                     Stochastic,
                     MACD)
#create data frame ICFFUT - 4/5 Arabica Coffee Futures
DFICFFUT <- data.frame(Date=index(Full_ICFFUT), coredata(Full_ICFFUT))
#Merge Data frame ICFFUT - 4/5 Arabica Coffee Futures and ICFFUT
CDFICFFUT <- left_join(DFICFFUT, 
                       ICFFUT |> select(Date, Decision, OpenInterest)) |>
  filter(Date > "2003-02-17") |> #excluding NA
  mutate(Decision  = as.factor(Decision)) #Decision as a factor = Y
```

The extensions are technical indicators based on a daily Close Price of ICF and are select based on Chartered Financial Analyst (CFA®) program curriculum. This choice make sense economically since this knowledge are widely disseminated to the market and the decisions are made at the closing call every day.

The decisions was build manually by the author, which look the daily chart and choose the optimal decision every day. As present in the model of chapter 27.8 Case study: is it a 2 or a 7? [@irizarry2019] the goal of this approach is to look at a picture and classifies what is the best decision at the closing call every day: Buy, Sell, Neutral. Is there a pattern inside the own prices of ICFFUT that bring some prediction?

## Data exploration and visualization

The exploration and visualization of the data provide insightful information about the behavior of price throughout time. The graphic below show the price range between 2003 and 2022, as we can see the volatility was different for different years.

```{r Chart ICFFUT Price, message=FALSE, warning=FALSE, echo=TRUE}
##Data exploration and visualization
#Chart ICFFUT Price
chart_Series(ICFFUT_XTS,type = "candlesticks")

```

A functional trade system implies that every day has at least one trade, in this case the days in which volume are zero we consider a no trade day. As presented below we have 4 days in this condition. For sake of simplicity we going to maintain this days because even without trade the adjust occur.

```{r Day without trade, message=FALSE, warning=FALSE, echo=TRUE}
#Day without trade
Notrade <- CDFICFFUT |> 
  filter(ICFFUT.Volume == "0") |>
  select(Date, ICFFUT.Close, ICFFUT.Volume, Decision)
Notrade
```

The proprieties of dataset reveal the possibilities and limitations of model approach. Lets check the class, the summary and the structure of data.

```{r class, type and proprieties of data frame, message=FALSE, warning=FALSE, echo=TRUE}
#class, type and proprieties of data frame
as_tibble(CDFICFFUT)
summary(CDFICFFUT)
str(CDFICFFUT)
```

As presented above our matrix have $4,435 \times 21$ variables, mostly numerical column. The close price, the principal column, has 119.6 as minimal value and 1206.0 as maximal, and a median of 470.3. The decision column, our outcome, has a unbalanced prevalence: $Buy = 1662$, $Neutral = 665$, $Sell = 2108$. Lets explore the distribution of decisions throughout years.

```{r distribution decision by year, message=FALSE, warning=FALSE, echo=TRUE}
#distribution decision by year
p1 <- CDFICFFUT |>
  mutate(year = year(as.Date(Date)))|>
  filter(Decision == "Buy") |>
  ggplot(aes(x = year)) +
  geom_bar(stat="count") + 
  ylab("Buy")

p2 <- CDFICFFUT |>
  mutate(year = year(as.Date(Date)))|>
  filter(Decision == "Neutral") |>
  ggplot(aes(x = year)) +
  geom_bar(stat="count") + 
  ylab("Neutral")

p3 <- CDFICFFUT |>
  mutate(year = year(as.Date(Date)))|>
  filter(Decision == "Sell") |>
  ggplot(aes(x = year)) +
  geom_bar(stat="count") + 
  ylab("Sell")


#Buy, Neutral, Sell distribution throughout years
gridExtra::grid.arrange(p1, p2, p3, 
                        nrow = 3, 
                        top = "Buy, Neutral, Sell distribution throughout years")
```

The graphs show different prevalence for different years, this a relevant information for building a general model, since the prevalence affect the precision and recall of model, and consequently the measurement of performance. The correlation between variables and outcome could bring more information about the behavior of data, which reflects the decisions of traders. To transform the decision from a factor to a number we replace $Neutral$ by $0$, $Buy$ for $1$, and $Sell$ for $-1$.

```{r correlation between variables and outcome, message=FALSE, warning=FALSE, echo=TRUE}
#correlation between variables and outcome
NDFICFFUT <- CDFICFFUT |> mutate( #Decision as a number
  NDecision = case_when(
    Decision == "Neutral" ~ 0, #If Neutral, then O
    Decision == "Buy" ~ 1,#If Buy, then 1
    Decision == "Sell" ~ -1)#If sell, then -1
  ) |> select(-Decision, -Date) #less categorical and date column
C <- cor(NDFICFFUT) #correlation matrix
corrplot(C, method = 'square', type = 'lower') 
```

The outcome are high correlated with extension fastK, which is 1 of 3 components of Stochastic Momentum Index. On the other side, the prices, the moving averages and the Bollinger Bands are low correlated with decisions. The volume has some correlations with Open interest, but almost nothing with others variables.

The result of decisions made throughout years, or the sum of profits, could be calculated using the code below. Considering the decision made at every day close call the potential return are the difference between today close price less the yesterday close price. Another important feature in this model is the status of daily position, which could be hold or change, either $Neutral$,$Buy$ or $Sell$.

The combination of status and current, or lag decision, determine the multiplier we use to calculate the adjust we going to receive or pay. Lets show a example to illustrate the idea, my decision last trade day was to be $Neutral$, and today is $Buy$, which means a change, the multiplier will be 0, because we have no adjust to pay or receive this day. In the next day we hold the $Buy$ position, then or multiplier will be 1 and we receive the positive change in price or we pay the negative difference in price. To complete the model we measure the sum of profit, daily adjust payed or received, for the last 22 days, which is a proxy of the monthly profit.

```{r empirical optimal results - sum last 22 days, message=FALSE, warning=FALSE, echo=TRUE}
#empirical optimal results - sum last 22 days
profitICFFUT <- CDFICFFUT |> 
  arrange(Date) |> 
  mutate(return = ICFFUT.Close - lag(ICFFUT.Close) ,
         status = as.factor(case_when(
           Decision == lag(Decision) ~ "Hold",
           Decision != lag(Decision) ~ "Change")),
         multiplier = case_when(
           status == "Change" & lag(Decision) == "Neutral" ~ 0,
           status == "Hold" & Decision == "Neutral" ~ 0,
           status == "Change" & lag(Decision) == "Buy" ~ 1,
           status == "Hold" & Decision == "Buy" ~ 1,
           status == "Change" & lag(Decision) == "Sell" ~ -1,
           status == "Hold" & Decision == "Sell" ~ -1),
         adjust = return * multiplier,
         profit = case_when(
           Decision == "Neutral" ~ 0,
           Decision != "Neutral" ~ sum_run(adjust, k=22)
           ))
summary(profitICFFUT |> select(return, status, adjust, profit))

```

The daily change, return, show the volatility of the asset, which reached 71.42 in one day, and has a median and mean below zero, which is compatible with the theory that the future price converge to spot price, or the tendency to lost value throughout time. The status was changed in 245 periods, or 5,5% of the days, this a important feature of the model that intended to minimize the trade cost. The profit, the ultimate goal of decision trade system, are consistently above zero and has median of 51.70 for the last 22 days. The annual profit could be verified in the table below, and show this median are too high for the last 5 years.

```{r profit by month and year, message=FALSE, warning=FALSE, echo=TRUE}
#profit adjust from the last 22 days
profitICFFUT |> 
  ggplot(aes(Date, profit)) + 
  geom_line()

#profit by year
profitY <- profitICFFUT |> mutate(Year = as.factor(year(as.Date(Date))))
profit_table <- as.data.frame(tapply(profitY$adjust, 
                                     profitY$Year, 
                                     FUN=sum, 
                                     na.rm = TRUE))
colnames(profit_table) <- "profit_year"
profit_table

```

## Modeling approach

The decision trade system is better as its errors has decrease, for this project the error has a categorical metrics evaluation. The summary metrics for this project is the F1-score, or balanced accuracy, that is the harmonic average of precision and recall. As presented in 27.4.5 Balanced accuracy and F1 score [@irizarry2019] "The $F1$ -score can be adapted to weigh specificity and sensitivity differently. To do this, we define $\beta$ to represent how much more important sensitivity is compared to specificity and consider a weighted harmonic average". For this project the $\beta = 1$, because the sensitivity has the same importance of specificity: $$
F1=\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\mbox{recall}} +
    \frac{1}{1+\beta^2}\frac{1}{\mbox{precision}} }
$$

For this project the $F1 > 0.50$ is the goal. The value was taken from (Coqueret and Guida 2020) where was suggested thata any accuracy above 50% may seem valuable.

### Machine Learning

*"If we have enough data, I know we can make predictions,"* **James Harris Simons**[^7]

[^7]: Zuckerman, Gregory. The Man Who Solved the Market (p. 2). Penguin Publishing Group. Kindle edition

The machine learning decisions are based on algorithms build with data, so for this project the dataset CDFICFFUT are going to be used to train and test the model. The training_sample and testing_sample are build by slicing the time, where the sample before 2020 are the training set and after the test set. The proportion of data for testing is 17% of total, in line with typical choice of 10%-20%[@irizarry2019]:

```{r create a partition, message=FALSE, warning=FALSE, echo=TRUE}
#Modeling approach
#create a partition
separation_date <- as.Date("2020-01-02")
training_sample <- filter(CDFICFFUT, Date < separation_date)
testing_sample <- filter(CDFICFFUT, Date >= separation_date)
```

### k-Nearest Neighbor Classification approach

The first model in this project is the k-nearest neighbors (kNN) approach, used by Renaissance Technologies and presented at chapter 29.1 Motivation with k-nearest neighbors [@irizarry2019]. The multiple dimensions adaptability is a important feature, specially in finance, to estimate $p(x_1, x_2)$:

$$
p(x_1, x_2) = \mbox{Pr}(Y=1 \mid X_1=x_1 , X_2 = x_2).
$$

As presented "for any point $(x1,x2)$ for which we want an estimate of $p(x1,x2)$, we look for the k nearest points to $(x1,x2)$ and then take an average of the 0s and 1s associated with these points. We refer to the set of points used to compute the average as the *neighborhood*." [@irizarry2019]. We going to use the knn3 function of caret package, with a $k=5$, to train the dataset. Then we show the first 5 rows of model by probability and class of decision.

```{r model 1, message=FALSE, warning=FALSE, echo=TRUE}
#defining the predictors - Model 1
knn_fit <- knn3(Decision ~ ., 
                data = training_sample, k=5)
knn_fit
#probability and class of model
head(predict(knn_fit, testing_sample, type = "prob"))
head(predict(knn_fit, testing_sample, type = "class"))
#balanced accuracy - model 1
y_hat_knn <- predict(knn_fit, testing_sample, type = "class")
confusionMatrix(y_hat_knn, testing_sample$Decision)$overall["Accuracy"]
cm1 <- confusionMatrix(y_hat_knn, testing_sample$Decision)
cm1[["byClass"]][ , "Precision"]
cm1[["byClass"]][ , "Recall"]
KNN <- cm1[["byClass"]][ , "F1"]
KNN
```

The accuracy of 0.4285714 is better than guessing, which is 0.3333, but is below our goal of 0.5. The F1 - score is below 0.5 for $Buy$ and $Sell$ decisions, and is zero for be $Neutral$. This could be a consequence of low prevalence of $Neutral$, or undetected pattern related with this decision.

### Recursive Partitioning for classification trees approach

The tree based method, presented in chapter 6 [@coqueret2020a], is our second model, and was chosen because has efficient forecast, are easy to visualize and could model human decision. The goal of classification tree is to split the dataset into homogeneous cluster, by minimizing dispersion inside each cluster. The output show the proportion of each class, in each cluster, in this case for $J$ classes, we denote these proportions with $p_j$, and for each cluster $k$, the usual loss functions are:

$$
\mbox{Gini}(j) = 
1-\sum_{j=1}^Jp_j^2;$$ $$
\mbox{entropy}(j) = -\sum_{k=1}^K \hat{p}_{j,k}\log(\hat{p}_{j,k}), \mbox{ with } 0 \times \log(0) \mbox{ defined as }0
$$

The first split is the most important of the model because show the most general rule of data aggregation and reveal the most relevant step to taken in trade decision. To implement this classification tree we going to use rpart function in the rpart package, with default complexity parameter.

```{r model 2, message=FALSE, warning=FALSE, echo=TRUE}
#defining the predictors - Model 2
fit <- rpart(Decision ~ ., data = training_sample)
rpart.plot(fit)
#Structural break
as.POSIXlt(1.514808e+09,origin="1970-01-01")
as.POSIXlt(1.438603e+09,origin="1970-01-01")
as.POSIXlt(1.319285e+09,origin="1970-01-01")
as.POSIXlt(1.251158e+09,origin="1970-01-01")

#accuracy - model 2
y_hat_RT <- predict(fit, testing_sample, type = "class")
confusionMatrix(y_hat_RT, testing_sample$Decision)$overall["Accuracy"]
cm2 <- confusionMatrix(y_hat_RT, testing_sample$Decision)
cm2[["byClass"]][ , "Precision"]
cm2[["byClass"]][ , "Recall"]
Classification_Tree <- cm2[["byClass"]][ , "F1"]
Classification_Tree
```

The general rule of trade decision in ICFFUT is to be $Sold$, but if the fastK is bigger than 0.69 the best decision is to change the decision to $Buy$. The $Buy$ has a proportion of 34% of decisions and has a 73% of probability of being right in this decision, this correspond to a F1-score of 0.55. The $Sell$ decision and $Neutral$ decision has structural breaks[^8], which means that decisions change throughout time. The $Sell$ decision has a F1-score of 0.60, much higher than our goal, but the $Neutral$ decision has no F1-score. This reveal the limitations of this approach, since the prevalence differ by class and by year, or could indicate the necessity to review the parameters.

[^8]: <https://en.wikipedia.org/wiki/Structural_break>

### Classification and Regression with Random Forest approach

A Random Forest is the last approach to solve the trade decision system problem presented in this project. The goal of this approach is to take the averaging of multiples simple trees to reduce the instability and improve the prediction performance, and the algorithm do this by bootstrapping the sample to induce randomness. The Random Forest used in this project follow the default setup, but the algorithm has more than 27 arguments and has 17 components values, which provides a wide range of tuning possibilities.

```{r model 3, message=FALSE, warning=FALSE, echo=TRUE}
#defining the predictors - Model 3
fit_RF <- randomForest(Decision ~., data = training_sample) 
#Variable Importance Plot
varImpPlot(fit_RF)
#accuracy - model 3
y_hat_RF <- predict(fit_RF, testing_sample, type = "class")
confusionMatrix(y_hat_RF, testing_sample$Decision)$overall["Accuracy"]
cm3 <- confusionMatrix(y_hat_RF, testing_sample$Decision)
cm3[["byClass"]][ , "Precision"]
cm3[["byClass"]][ , "Recall"]
Random_Forest <- cm3[["byClass"]][ , "F1"]
Random_Forest
```

The model reveal that the most important variable is the fastK, followed by pctB, which is a component of Bollinger Bands indicator and quantifies the price relative to the upper and lower Bollinger Band. The Date is a important variable too, showing that model most to adapt to different time frame. The F1 score of $Neutral$ decision is 0.013, which is very low, but it´s not null, showing some improvement regarding the two models before. The $Sell$ decision has a F1 score of 0.58, above our goal, but smaller than the model of tree classification. The improvement in $Neutral$ decision come with a cost to $Buy$ decision that has a F1 score of 0.45 for this model, below our goal.

# Results

This section presents the modeling results and discusses the performance of each approach. The table below shows the F1 score by decision and by approach.

```{r Results, message=FALSE, warning=FALSE, echo=TRUE}
#Results
knitr::kable(tibble(Decision = c("Buy", "Neutral", "Sell"), 
                    KNN, 
                    Classification_Tree, 
                    Random_Forest), 
             caption = "F1 Score by approach",
             position = "h")
```

The highest F1 score for $Buy$ and $Sell$ are achieved using classification tree, and are above our goal of 50%. The $Sell$ decision has the best result, which could be influence of the highest prevalence. The $Neutral$ decision was predicted only by random forest approach, and has a poor performance F1 score that is below 3%. The profit, or loss, of the test set was not presented since the true out-of-sample tests are only possible in live trading.

# Conclusion

The decision traded system, developed in this project, provide suggestions for decision that are most pertinent to a particular combination of close price and their own extensions. The goal of this project was partially achieved and the $F1 score$ of $Buy$ and $Sell$ is above 50%, our goal, using Classification Tree . The add of other assets and economic variable could provide a even better model since we have more data to train, and can capture more structures and latent factors in the data.

# References

Zuckerman, Gregory. 2019. The Man Who Solved the Market, Penguin Publishing Group. Kindle edition
